subtitle = "(Sentiment Contribution By Word)",
x = "Word", y = "Sentiment Contribution")
speech_bing_count %>% filter(n >= 2) %>%
mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
mutate(pct_contr = n / sum(n)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, pct_contr, fill = sentiment)) +
geom_bar(stat = "identity", width = .95) +
josi_theme() + scale_fill_manual(values = c("#ff879e", "#87bdff"),
name = "Sentiment",
labels = c("Negative", "Positive")) +
coord_flip() +
labs(title = "Donald Trump's Acceptance Speech",
subtitle = "(Sentiment Contribution By Word)",
x = "Word", y = "Sentiment Contribution")
rm(list = ls())
library(dplyr)
library(ggplot2)
library(stringr)
library(tidytext)
#read in acceptance speach raw, sep is line break, remove quoting, strings are not factors
speech_hrc <- read.table("/Users/Josiah/Dropbox/general_R/Data/clinton_concession.txt", sep = "\n", stringsAsFactors = F, quote = "", col.names = c("text"))
# add line number, author, speech name, and the date of the speech (just for future reference)
speech_hrc <- speech_hrc %>% mutate(line = row_number(), author = "Hillary Clinton", speech = "2016 Presidential Election Concession Speech", date = as.POSIXct("2016-11-10"))
# Load stop words
data("stop_words")
# Convert sentences to words
speech_hrc <- speech_hrc %>% unnest_tokens(word, text)
# Remove stop words
speech_hrc <- speech_hrc %>% anti_join(stop_words)
# word count
word_count <- speech_hrc %>% count(word, sort = T)
### SENTIMENT ANALYSIS OF CLINTON CONCESSION SPEECH
afinn <- get_sentiments("afinn")
# Bing can measure overall positive / negative sentiment over time
bing <- get_sentiments("bing")
# NRC is used to find feelings such as "disgust" "trust" "anger" etc.
# Can search for just one sentiment.
nrc <- get_sentiments("nrc")
# Sentiment contribution by word for HRC
speech_bing_count_hrc <- inner_join(speech_hrc, bing, by = "word") %>%
count(word, sentiment, sort = T) %>% ungroup()
View(speech_bing_count_hrc)
View(speech_hrc)
set_wd
setwd("Dropbox")
setwd("/Dropbox")
getwd
getwd()
write.csv(speech_hrc, "/Users/Josiah/Dropbox/general_R/Data/hrc_conession.csv")
rm(lists = ls())
library(tidyr)
library(dplyr)
library(ggplot2)
library(stringr)
library(tidytext)
#read in acceptance speach raw, sep is line break, remove quoting, strings are not factors
speech <- read.table("/Users/Josiah/Dropbox/general_R/Data/trump_acceptance.txt", sep = "\n", stringsAsFactors = F, quote = "", col.names = c("text"))
# add line number, author, speech name, and the date of the speech (just for future reference)
speech <- speech %>% mutate(line = row_number(), author = "Donald Trump", speech = "2016 Acceptance Speech", date = as.POSIXct("2016-11-09"))
# load tidy text
library(tidytext)
data("stop_words")
#change tokens
speech <- speech %>% unnest_tokens(word, text)
# remove stop words
speech <- speech %>% anti_join(stop_words)
speech <- speech %>% anti_join(stop_words)
View(speech)
write.csv(speech, "/Users/Josiah/Dropbox/general_R/Data/trump_acceptance.csv")
write.csv(rbind(speech,speech_hrc), "/Users/Josiah/Dropbox/general_R/Data/hrc_trump_tidy.csv")
rm(list = ls())
#devtools::install_github('dgrtwo/widyr')
library(widyr)
library(dplyr)
library(tidytext)
library(tidyr)
# Read in trump speech
trump <- read.table("/Users/Josiah/Dropbox/general_R/Data/trump_acceptance.txt",
stringsAsFactors = F, quote = "", col.names = c("text"), sep = "\n")
View(trump)
#devtools::install_github('dgrtwo/widyr')
library(widyr)
library(dplyr)
library(tidytext)
library(tidyr)
# Read in trump speech
trump <- read.table("/Users/Josiah/Dropbox/general_R/Data/trump_acceptance.txt",
stringsAsFactors = F, quote = "", col.names = c("text"), sep = "\n")
# Creating bi-grams (pairs of words (2 words))
trump_bigrams <- trump %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
trump_bigrams %>%
count(bigram, sort = T)
# Since there are a number of bigrams that have common stop words in them
# We can separate into their component words and remove stop words using joins
# Separate bi-grams
bigrams_sep <- trump_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
# Filter out stop words
bigrams_clean <- bigrams_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# Count common word pairs
bigrams_clean %>%
count(word1, word2, sort = T)
# Rejoin the bigrams
bigrams_unite <- bigrams_clean %>%
unite(bigram, word1, word2, sep = " ")
source('~/Dropbox/general_R/text_analysis_bookwork/ch4_bigrams.R', echo=TRUE)
View(trump_bigrams)
rm(list = ls())
#devtools::install_github('dgrtwo/widyr')
library(widyr)
library(dplyr)
library(tidytext)
library(tidyr)
# Read in trump speech
trump <- read.table("/Users/Josiah/Dropbox/general_R/Data/trump_acceptance.txt",
stringsAsFactors = F, quote = "", col.names = c("text"), sep = "\n")
# Creating bi-grams (pairs of words (2 words))
trump_bigrams <- trump %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
trump_bigrams %>%
count(bigram, sort = T)
# Since there are a number of bigrams that have common stop words in them
# We can separate into their component words and remove stop words using joins
# Separate bi-grams
bigrams_sep <- trump_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
# Filter out stop words
bigrams_clean <- bigrams_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# Count common word pairs
bigrams_clean %>%
count(word1, word2, sort = T)
# Rejoin the bigrams
bigrams_unite <- bigrams_clean %>%
unite(bigram, word1, word2, sep = " ")
book_words <- austen_books() %>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE) %>%
ungroup()
total_words <- book_words %>%
group_by(book) %>%
summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
book_words
book_words <- austen_books() %>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE) %>%
ungroup()
library(janeaustenr)
book_words <- austen_books() %>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE) %>%
ungroup()
book_words
book_words %>%
group_by(book) %>%
summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
total_words <- book_words %>%
group_by(book) %>%
summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
book_words
bigrams_unite %>%
count(bigram) %>%
bind_tf_idf(bigram, n) %>%
arrange(desc(tf_idf))
bigrams_unite %>%
count(bigram)
bigrams_unite %>%
count(bigram) %>%
bind_tf_idf(bigram, n)
bigrams_unite %>%
count(bigram) %>%
bind_tf_idf(bigram, x, n)
?bind_tf_idf
bigrams_unite %>%
count(bigram) %>%
mutate(author = "Donald Trump") %>%
bind_tf_idf(bigram, author, n) %>%
arrange(desc(tf_idf))
rm(list = ls())
install.packages("neuralnet")
library(neuralnet)
infert
neuralnet
?apply
df <- data.frame(state = c("NH","NH","NH","NH","NH", "VT","VT","VT","VT","VT"),
year = c(2009, 2030, 2050, 2070, 2090),
revenu = c(451.1,300.98,329.97,309.08,645.63,691.75,559.99,513.87))
df <- data.frame(state = c("NH","NH","NH","NH","NH", "VT","VT","VT","VT","VT"),
year = c(2009, 2030, 2050, 2070, 2090, 2009, 2030, 2050, 2070, 2090),
revenu = c(451.1,300.98,329.97,309.08,645.63,691.75,559.99,513.87))
df <- data.frame(state = c("NH","NH","NH","NH","NH", "VT","VT","VT","VT","VT"),
year = c(2009, 2030, 2050, 2070, 2090, 2009, 2030, 2050, 2070, 2090),
revenu = c(451.1, 300.98, 329.97, 309.08, 645.63, 691.75, 559.99, 513.87,400,300))
library(ggplot2)
df %>% ggplot(aes(x= year, y = year, color = state)) %>% geom_line()
library(dplyr)
df %>% ggplot(aes(x= year, y = year, color = state)) %>% geom_line()
ggplot(df, aes(x= year, y = year, color = state)) %>% geom_line()
ggplot(df, aes(x= year, y = year) %>% geom_line()
ggplot(df, aes(x= year, y = year)) %>% geom_line()
ggplot(df, aes(x= year, y = year)) %>% geom_line()
ggplot(df, aes(year, revenu)) %>% geom_line()
df <- data.frame(state = c("NH","NH","NH","NH","NH", "VT","VT","VT","VT","VT"),
year = c(2009, 2030, 2050, 2070, 2090, 2009, 2030, 2050, 2070, 2090),
revenue = c(451.1, 300.98, 329.97, 309.08, 645.63, 691.75, 559.99, 513.87,400,300))
ggplot(df, aes(year, revenu)) %>% geom_line()
revenue = c(451.1, 300.98, 329.97, 309.08, 645.63, 691.75, 559.99, 513.87,400,300))
ggplot(df, aes(year, revenue)) %>% geom_line()
ggplot(df, aes(year, revenue)) %>% geom_line()
ggplot(df, aes(year, revenue)) %>% geom_line()
ggplot(df, aes(year, revenue)) + geom_line()
ggplot(df, aes(year, revenue, color = state)) + geom_line()
♠
x <- ♠
rm(list = ls())
read.table("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
library(readr)
read_file("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
read_lines("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
guess_encoding("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
stri_enc_detect("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
library(stringi)
stri_enc_detect("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
guess_encoding("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", n_max = -1, threshold = 0)
x <- guess_encoding("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", n_max = -1, threshold = 0)
View(x)
x[1]
x[[1]]
stri_enc_detect("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
y <- stri_enc_detect("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
y[1]
library(broom)
tidy(y)
y[[1]]
y[[1]][1]
y[1][1]
y[[2]]
y[2]
y[3]
y[[2]]
?stri_conv
str_conv(stri_enc_get("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp"), to = 'UTF-8')
stri_conv(stri_enc_get("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp"), to = 'UTF-8')
stri_conv(stri_enc_get(x = "/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp"), to = 'UTF-8')
stri_conv(str= ("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp"),
from = stri_enc_get("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp"),
to = 'UTF-8')
stri_enc_get("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
?stri_enc_get
stri_conv(str= ("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp"),
from = "windows-1250",
to = 'UTF-8')
read_lines_raw("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
?read_lines_raw
read_lines_raw("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", locale(encoding = "windows-1250"))
read_lines_raw("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", locale = (encoding = "windows-1250"))
read_lines_raw("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp",
locale = locale(encoding = "windows-1250")
)
read_lines_raw("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp",
locale = locale(encoding = "windows-1250"))
read_lines_raw("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp",
locale = locale(encoding = "Windows-1250"))
read_lines_raw("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp",
locale = locale(encoding = "Windows-1252"))
read_lines_raw("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
y
read_lines_raw("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", locale = encoding("ISO-8859-1"))
?
?readLines
readLines("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", encoding = "WINDOWS-1252")
readLines("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", encoding = "ISO-8859-1")
readLines("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", encoding = "unkown")
readLines("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", encoding = "unknown")
read_lines_raw("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", locale = encoding("UTF-8"))
readL("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", encoding = "unkown")
readL("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", encoding = "unknown")
readLines("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", encoding = "unknown")
readLines("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", encoding = "Windows-1252")
?read_lines_raw
?locale
read_lines("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp", locale = locale(date_names = "fr"))
locale = locale(date_names = "fr", enoding = "ISO-8859-1"))
read_lines("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp",
locale = locale(date_names = "fr", enoding = "ISO-8859-1"))
read_lines("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp",
locale = locale(date_names = "fr", encoding = "ISO-8859-1"))
read_lines("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp",
locale = locale(date_names = "fr", encoding = "Windows-1250"))
read_lines("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp",
locale = locale(date_names = "fr", encoding = "Windows-1252"))
read_lines("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp",
locale = locale(date_names = "fr", encoding = "WINDOWS-1252"))
data.restore("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
library(foreign)
data.restore("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
read_lines_raw("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
read_lines_raw("/Volumes/GIS/LRPC/2013/06032013/TPD130603105551.dmp")
rm(list = ls())
rm(list = ls())
library(c("shiny", "shinydashboard"))
library(shiny)
library(shinydashboard)
runGitHub("stats-apps", "datacamp", subdir = "best-fit")
install.packages("openintro")
runGitHub("stats-apps", "datacamp", subdir = "best-fit")
install.package("oilabs")
install.packages("oilabs")
runGitHub("stats-apps", "datacamp", subdir = "best-fit")
# Set up your working directory
#setwd("~/Documents/github/house_expenditures")
# Download and read Q3 detail files
file_downloads <- c("https://pp-projects-static.s3.amazonaws.com/congress/staffers/2016Q3-house-disburse-detail.csv",
"https://pp-projects-static.s3.amazonaws.com/congress/staffers/2015Q3-house-disburse-detail.csv",
"https://pp-projects-static.s3.amazonaws.com/congress/staffers/2014Q3-house-disburse-detail.csv",
"https://pp-projects-static.s3.amazonaws.com/congress/staffers/2013Q3-house-disburse-detail.csv",
"https://pp-projects-static.s3.amazonaws.com/congress/staffers/2012Q3-house-disburse-detail.csv",
"https://pp-projects-static.s3.amazonaws.com/congress/staffers/2011Q3-house-disburse-detail.csv",
"https://pp-projects-static.s3.amazonaws.com/congress/staffers/2010Q3-house-disburse-detail.csv",
"https://pp-projects-static.s3.amazonaws.com/congress/staffers/2009Q3-house-disburse-detail.csv"
)
alldata <- lapply(file_downloads, function(x) {read.csv(x, stringsAsFactors = F)})
df <- do.call(rbind, alldata) # Combine into one dataset
# Convert to 2016 dollars
# q = quarter as character
# r = inflation rate found on http://www.usinflationcalculator.com/
convertDollars <- function(q, r) {
round(df$AMOUNT[df$QUARTER == q] + (df$AMOUNT[df$QUARTER == q] * r), 2)
}
df$AMOUNT[df$QUARTER == "2009Q3"] <- convertDollars("2009Q3", .125) #Convert 2009
df$AMOUNT[df$QUARTER == "2010Q3"] <- convertDollars("2010Q3", .107) #Convert 2010
df$AMOUNT[df$QUARTER == "2011Q3"] <- convertDollars("2011Q3", .073) #Convert 2011
df$AMOUNT[df$QUARTER == "2012Q3"] <- convertDollars("2012Q3", .051) #Convert 2012
df$AMOUNT[df$QUARTER == "2013Q3"] <- convertDollars("2013Q3", .036) #Convert 2013
df$AMOUNT[df$QUARTER == "2014Q3"] <- convertDollars("2014Q3", .02) #Convert 2014
df$AMOUNT[df$QUARTER == "2015Q3"] <- convertDollars("2015Q3", .018) #Convert 2015
# Change class and remove commas
df$AMOUNT <- as.numeric(gsub(",", "", df$AMOUNT))
# Convert date variables to dates
df$START.DATE <- as.Date(df$START.DATE, format = "%m/%d/%y")
df$END.DATE <- as.Date(df$END.DATE, format = "%m/%d/%y")
# Convert to 2016 dollars
# q = quarter as character
# r = inflation rate found on http://www.usinflationcalculator.com/
convertDollars <- function(q, r) {
round(df$AMOUNT[df$QUARTER == q] + (df$AMOUNT[df$QUARTER == q] * r), 2)
}
df$AMOUNT[df$QUARTER == "2009Q3"] <- convertDollars("2009Q3", .125) #Convert 2009
df$AMOUNT[df$QUARTER == "2010Q3"] <- convertDollars("2010Q3", .107) #Convert 2010
df$AMOUNT[df$QUARTER == "2011Q3"] <- convertDollars("2011Q3", .073) #Convert 2011
df$AMOUNT[df$QUARTER == "2012Q3"] <- convertDollars("2012Q3", .051) #Convert 2012
df$AMOUNT[df$QUARTER == "2013Q3"] <- convertDollars("2013Q3", .036) #Convert 2013
df$AMOUNT[df$QUARTER == "2014Q3"] <- convertDollars("2014Q3", .02) #Convert 2014
df$AMOUNT[df$QUARTER == "2015Q3"] <- convertDollars("2015Q3", .018) #Convert 2015
View(df)
df %>% filter(PAYEE == RECIP..orig..)
gc()
rm(list = ls())
gc()
.rs.restartR()
gc()
rm(list = ls())
gc()
.rs.restartR()
read.csv("/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt")
read.table("/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt")
read.lines("/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt")
read_lines("/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt")
readLines"/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt")
readlines"/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt")
readlines("/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt")
readline("/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt")
readLine("/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt")
readLines("/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt")
library(tidyverse)
read_delim("/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt")
read_lines("/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt")
x <- read_lines("/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt")
x
x[1]
x <- read_delim("/Users/Josiah/Google_Drive/College/Spring_2017/Scientific Programming/Homework/data/2016boydwx.txt", "\t")
x
View(x)
rm(list = ls())
footprint_per_bag * 3
# The average plastic grocery bag weighs 5.5kg (https://alumni.stanford.edu/get/page/magazine/article/?article_id=30162)
# Carbon footprint of 1kg of plastic is ~6kg CO2 (http://timeforchange.org/plastic-bags-and-plastic-bottles-CO2-emissions)
footprint <- 6 #kg CO2per kg of plastic
grocery <- 5.5 #kg of plastic
footprint_per_bag <- grocery * footprint #kg of CO2
# Say we estimate we use 3 grocery bags and could have fit all in our one reusable canvas bag
total_bag_footprint <- footprint_per_bag * 3
footprint_per_bag * 3
source('~/.active-rstudio-document', echo=TRUE)
library(tutorial)
?tutorial
??tutorial
bag_footprint
knitr::opts_chunk$set(echo = TRUE)
tutorial::go_interactive()
footpring_kg <- 6
bag_mass <- 5.5
bag_footprint <- footpring_kg * bag_mass
bag_footprint <- footpring_kg * bag_mass
tutorial::go_interactive()
bag_footprint
knitr::opts_chunk$set(echo = TRUE)
footpring_kg <- 6
bag_mass <- 5.5
bag_footprint <- footpring_kg * bag_mass
print(bag_footprint, "kg")
?princomp
footpring_kg <- 6
bag_mass <- 5.5
bag_footprint <- footpring_kg * bag_mass
paste(bag_footprint, "kg")
ethanol_emiss <- 17.68 * 0.453592
gallons_one_way <- 1.7/20
emiss_one_way <- gallons_one_way * ethanol_emiss
paste(round(emiss_one_way, 4), "kg of CO2 emitted in a one way trip")
ethanol_emiss <- 17.68 * 0.453592
gallons_one_way <- 1.7/20
emiss_one_way <- gallons_one_way * ethanol_emiss
paste(round(emiss_one_way, 3), "kg of CO2 emitted in a one way trip")
# Account for bad catalytic converter
emiss_one_way <- emiss_one_way * 1.2
# Find my actual footprint
actual_footprint <- bag_footprint * 3 + emiss_one_way * 2
# Find alternative footprint
alternative_footprint <- emiss_one_way * 4
# Test for significant difference
t.test(actual_footprint, alternative_footprint)
?t.test
actual_footprint - alternative_footprint
alternative_footprint
actual_footprint
actual_footprint
# Account for bad catalytic converter
emiss_one_way <- emiss_one_way * 1.2
# Find my actual footprint
actual_footprint <- bag_footprint * 3 + emiss_one_way * 2
# Find alternative footprint
alternative_footprint <- emiss_one_way * 4
actual_footprint
alternative_footprint
footprint_kg <- footprint_kg - 2.5
footprint_kg <- 6
bag_mass <- 5.5
bag_footprint <- footprint_kg * bag_mass
paste("Footprint per bag is", bag_footprint, "kg.")
footprint_kg <- footprint_kg - 2.5
bag_footprint <- footprint_kg * bag_mass
paste("Total footprint for 3 bags is", bag_footprint * 3, "kg")
ethanol_emiss <- 17.68 * 0.453592
gallons_one_way <- 1.7/20
emiss_one_way <- gallons_one_way * ethanol_emiss
paste(round(emiss_one_way, 3), "kg of CO2 emitted in a one way trip")
bag_footprint * 3
emiss_one_way * 2
actual_footprint <- bag_footprint * 3 + emiss_one_way * 2
actual_footprint
actual_footprint - alternative_footprint
rm(list = ls())
library(tidyverse)
library(data.table)
getwd()
setwd("/Users/Josiah/Dropbox/GitHub/russia-investigation/jeff-sessions")
hearing <- read_delim("data/transcript_raw.txt",
delim = "\n", col_names = "line")
library(tidyverse)
library(stringr)
hearing <- read_delim("data/transcript_raw.txt",
delim = "\n", col_names = "line")
View(hearing)
library(tidyverse)
library(stringr)
hearing <- read_delim("data/transcript_raw.txt",
delim = "\n", col_names = "line")
View(hearing)
# Create index to replace (UNKNOWN): with UNKNOWN: to match speaker names from rest
unknown_index <- str_detect(hearing$line, "\\([A-Z]+\\):")
hearing$line[unknown_index] <- str_replace_all(hearing$line[unknown_index], "\\(|\\)", "")
# Detect the speakers of each line
speaker <- str_extract_all(hearing$line, "[A-Z]+:")
# initialize vector to fill with speaker that will be bound to hearing
to_fill <- rep(NA, 1,length(speaker))
# Detect the speakers of each line
speaker <- str_extract_all(hearing$line, "[A-Z]+:")
# initialize vector to fill with speaker that will be bound to hearing
to_fill <- rep(NA, 1,length(speaker))
# Loop to identify speaker for each line
for (i in 1:length(speaker)) {
if (identical(speaker[[i]], character(0)) == FALSE) {
temp_speaker <- str_replace(speaker[[i]], "\\:", "")
}
to_fill[i] <- temp_speaker
}
# Create tibble with speaker
hearing <- tibble(line = hearing$line, speaker = to_fill) %>%
mutate(line_num = row_number(),
date = as.Date("2017-06-13"))
hearing$line <- str_replace_all(hearing$line, "[A-Z]+:", "")
write_csv(hearing, "data/sessions_hearing.csv")
View(hearing)
